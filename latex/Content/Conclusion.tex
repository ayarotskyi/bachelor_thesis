\chapter{Conclusion and Future Work}
\label{cha:Conclusion}

\section{Conclusion}

Addressing the main research questions:

\begin{enumerate}
  \item \textbf{Can the model be trained using Behavioral Cloning to demonstrate human performance or even surpass it in the first difficulty?} \\
    The evaluation results demonstrate that the behavioral cloning model successfully replicated the expert’s navigational behavior to a considerable degree. The relatively low Davies–Bouldin index and moderate silhouette scores indicate that BCNet produced trajectories with structural patterns comparable to those of the expert.

    However, while BCNet with LSTM layers performs decently in the early stages of the course due to its conservative control, it struggles in later phases, where minor prediction errors compound. This suggests that more diverse training data or alternative methods, such as DAgger, may be necessary to enhance robustness in varied environments.

  \item \textbf{Can the model be sufficiently generalized using behavioral cloning to demonstrate human performance or even surpass it in the second difficulty?} \\
    Unfortunately, the success rate of the BCNet-driven JetBot was 0\% under the conditions of the second difficulty level. This clearly indicates that the model failed to generalize to spatial variations it had not encountered during training. This limitation is a well-known challenge in behavioral cloning (BC) \autoref{NIPS1988_812b4ba2}. Since BC relies heavily on supervised learning from expert trajectories, it tends to perform poorly when encountering states that lie outside of the training distribution. The lack of corrective feedback during training means the model is ill-equipped to recover from novel or slightly off-distribution states, especially in tasks involving obstacle avoidance from unfamiliar angles or trajectories.

  \item \textbf{Can the Inverse Reinforcement Learning (IRL) approach eliminate the expected weaknesses of the Behavioral Cloning approach in mastering the course on both difficulty levels and contribute to successfully mastering the course?} \\
    As this thesis did not provide an opportunity to explore this question empirically, potential solutions will be discussed in \autoref{sec:future_work}.
\end{enumerate}

\section{Future Work}
\label{sec:future_work}

Based on the findings and limitations of this study, several clear directions for future research can be identified to improve the performance and generalization:

\begin{enumerate}
  \item \textbf{Increase the Size and Variety of Training Data} \\
    To improve the model’s ability to generalize, a larger dataset should be collected. Especially the dataset for the second difficulty should include a wider range of initial conditions, obstacle arrangements, and environmental variations. Previous research \autocite{KALRA2016182} shows that the amount of high-quality data needed for autonomous vehicle to operate with decent precision should be very high.

  \item \textbf{Use More Robust Training Methods like Dataset Aggregation (DAgger)} \\
    Since behavioral cloning struggles with unfamiliar states and error accumulation, another possible direction is to explore interactive learning methods like DAgger \autocite{ross2011reduction}. This approach allows the model to receive expert corrections on its own mistakes during training, helping it learn to recover from new or unexpected situations. DAgger could specifically address the compounding errors observed in later course sections. For example, in \autocite{pan2019agileautonomousdrivingusing} a MPPI system that operates using an expensive set of sensors is utilized to collect the data and to train a convolutional neural model to operate on a similar precision by relying only on the camera input.

  \item \textbf{Combine Deep Learning with Model Predictive Path Integral (MPPI) Control} \\
    Integrating deep neural networks with MPPI control methods offers another way to improve navigation. Recent works \autocite{lee2021approximateinversereinforcementlearning, drews2017aggressivedeepdrivingmodel} show that convolutional neural networks can predict cost maps used by MPPI controllers to plan better trajectories. This combination leverages learned perception and model-based planning to handle complex and dynamic environments more effectively.

  \item \textbf{Apply Inverse Reinforcement Learning (IRL) to Learn Reward Functions} \\
    Future research could use inverse reinforcement learning to derive reward functions from expert behavior. Neural networks can be trained to estimate rewards based on state information such as speed, position, and distances to obstacles. These learned rewards can then be used to train policies with on-policy or off-policy reinforcement learning. While on-policy training may require simulation, off-policy methods allow learning from existing data \autocite{arnob2020off}.

\end{enumerate}
